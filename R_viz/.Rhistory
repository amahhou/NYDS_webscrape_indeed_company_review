summary(model.saturated) #Many predictor variables are not significant, yet the
plot(model.saturated) #Assessing the assumptions of the model.
library(car) #Companion to applied regression.
influencePlot(model.saturated)
vif(model.saturated) #Assessing the variance inflation factors for the variables
avPlots(model.saturated) #Distinct patterns are indications of good contributions
model2 = lm(Life.Exp ~ . - Illiteracy, data = states)
summary(model2) #R^2 adjusted went up, model still significant, etc.
plot(model2) #No overt additional violations.
influencePlot(model2) #No overt additional violations; Hawaii actually lowers
vif(model2) #VIFs all decrease.
anova(model2, model.saturated) #The p-value is quite large, indicating that we
model.full = lm(Life.Exp ~ ., data = states)
model.reduced = lm(Life.Exp ~ . - Illiteracy - Area - Income, data = states)
anova(model.reduced, model.full) #The p-value is quite large; thus, the reduced
summary(model.reduced)
plot(model.reduced)
influencePlot(model.reduced)
vif(model.reduced)
AIC(model.full,    #Model with all variables.
model2,        #Model with all variables EXCEPT Illiteracy.
model.reduced) #Model with all variables EXCEPT Illiteracy, Area, and Income.
BIC(model.full,
model2,
model.reduced) #Both the minimum AIC and BIC values appear alongside the
model.empty = lm(Life.Exp ~ 1, data = states) #The model with an intercept ONLY.
model.full = lm(Life.Exp ~ ., data = states) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
library(MASS) #The Modern Applied Statistics library.
forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
backwardAIC = step(model.full, scope, direction = "backward", k = 2)
bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)
bothAIC.full = step(model.full, scope, direction = "both", k = 2)
forwardBIC = step(model.empty, scope, direction = "forward", k = log(50))
backwardBIC = step(model.full, scope, direction = "backward", k = log(50))
bothBIC.empty = step(model.empty, scope, direction = "both", k = log(50))
bothBIC.full = step(model.full, scope, direction = "both", k = log(50))
summary(forwardAIC)
plot(forwardAIC)
influencePlot(forwardAIC)
vif(forwardAIC)
avPlots(forwardAIC)
confint(forwardAIC)
forwardAIC$fitted.values #Returns the fitted values.
newdata = data.frame(Murder = c(1.5, 7.5, 12.5),
HS.Grad = c(60, 50, 40),
Frost = c(75, 55, 175),
Population = c(7500, 554, 1212))
predict(forwardAIC, newdata, interval = "confidence") #Construct confidence intervals
predict(forwardAIC, newdata, interval = "prediction") #Construct prediction invervals
library(caret)
train(Life.Exp~., data=states,
method="glmStepAIC", k=2,
trControl=trainControl(method="none"))
library(dplyr)
states %>% train(Life.Exp~., data=.,
method="glmStepAIC", k=2,
trControl=trainControl(method="none"))
train(Life.Exp~., data=states,
method="glmStepAIC", k=2,
trControl=trainControl(method="none"), preProc=c("center", "scale"))
predict.train(mod, states)
mod = train(Life.Exp~., data=states,
method="glmStepAIC", k=2,
trControl=trainControl(method="none"), preProc=c("center", "scale"))
predict.train(mod, states)
state1 = sapply(states, function(x) (x-mean(x))/sd(x))
mod = train(Life.Exp~., data=state1,
method="glmStepAIC", k=2,
trControl=trainControl(method="none"))
predict.train(mod, state1)
state1 = sapply(states %>% select(-age), function(x) (x-mean(x))/sd(x))
state1 = sapply(states %>% select(-Life.Exp), function(x) (x-mean(x))/sd(x))
mod = train(Life.Exp~., data=state1,
method="glmStepAIC", k=2,
trControl=trainControl(method="none"))
tests = read.table("[04] Test Scores.txt")
summary(tests)
sd(tests$Test.Score)
sd(tests$Hours.Studied)
cor(tests$Test.Score, tests$Hours.Studied)
plot(tests$Hours.Studied, tests$Test.Score)
model.simple = lm(Test.Score ~ Hours.Studied, data = tests)
summary(model.simple) #Investigating the model and assessing some diagnostics.
plot(model.simple)
influencePlot(model.simple)
newdata = data.frame(Hours.Studied = seq(1, 3, length = 100))
conf.band = predict(model.simple, newdata, interval = "confidence")
pred.band = predict(model.simple, newdata, interval = "prediction")
plot(tests$Hours.Studied, tests$Test.Score,
xlab = "Hours Studied", ylab = "Test Score",
main = "Simple Linear Regression Model\nTests Dataset")
abline(model.simple, lty = 2)
lines(newdata$Hours.Studied, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Hours.Studied, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Hours.Studied, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Hours.Studied, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
model.quadratic = lm(Test.Score ~ Hours.Studied + I(Hours.Studied^2), data = tests)
summary(model.quadratic) #Investigating the model and assessing some diagnostics.
plot(model.quadratic)
conf.band = predict(model.quadratic, newdata, interval = "confidence")
pred.band = predict(model.quadratic, newdata, interval = "prediction")
plot(tests$Hours.Studied, tests$Test.Score,
xlab = "Hours Studied", ylab = "Test Score",
main = "Quadratic Regression Model\nTests Dataset")
lines(tests$Hours.Studied[order(tests$Hours.Studied)],
model.quadratic$fitted.values[order(tests$Hours.Studied)], lty = 2)
lines(newdata$Hours.Studied, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Hours.Studied, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Hours.Studied, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Hours.Studied, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
model.factor = lm(Test.Score ~ Hours.Studied + Gender, data = tests)
summary(model.factor) #Investigating the model and assessing some diagnostics.
plot(model.factor)
col.vec = c(rep("pink", 250), rep("blue", 250))
plot(tests$Hours.Studied, tests$Test.Score, col = col.vec,
xlab = "Hours Studied", ylab = "Test Score",
main = "Linear Regression Model w/ Factor\nTests Dataset")
abline(model.factor$coefficients[1], #Intercept for females.
model.factor$coefficients[2], #Slope for females.
lwd = 3, lty = 2, col = "pink")
abline(model.factor$coefficients[1] + model.factor$coefficients[3], #Intercept for males.
model.factor$coefficients[2], #Slope for males.
lwd = 3, lty = 2, col = "blue")
legend("topleft", c("Female Regression", "Male Regression"),
lwd = 3, lty = 2, col = c("pink", "blue"))
setwd("C:/Users/dsp21/NYDS/Lectures/Stats & ML/[04] Multiple Linear Regression Lecture Code")
help(state.x77)
state.x77 #Investigating the state.x77 dataset.
states = as.data.frame(state.x77) #Forcing the state.x77 dataset to be a dataframe.
colnames(states)[4] = "Life.Exp"
colnames(states)[6] = "HS.Grad"
states[,9] = (states$Population*1000)/states$Area
colnames(states)[9] = "Density"
summary(states)
sapply(states, sd)
cor(states)
plot(states)
model.saturated = lm(Life.Exp ~ ., data = states)
summary(model.saturated) #Many predictor variables are not significant, yet the
plot(model.saturated) #Assessing the assumptions of the model.
library(car) #Companion to applied regression.
influencePlot(model.saturated)
vif(model.saturated) #Assessing the variance inflation factors for the variables
avPlots(model.saturated) #Distinct patterns are indications of good contributions
?avPlots
avPlots(model.saturated) #Distinct patterns are indications of good contributions
model2 = lm(Life.Exp ~ . - Illiteracy, data = states)
summary(model2) #R^2 adjusted went up, model still significant, etc.
plot(model2) #No overt additional violations.
influencePlot(model2) #No overt additional violations; Hawaii actually lowers
vif(model2) #VIFs all decrease.
anova(model2, model.saturated) #The p-value is quite large, indicating that we
anova(model2, model.saturated) #The p-value is quite large, indicating that we
model.full = lm(Life.Exp ~ ., data = states)
model.reduced = lm(Life.Exp ~ . - Illiteracy - Area - Income, data = states)
anova(model.reduced, model.full) #The p-value is quite large; thus, the reduced
summary(model.reduced)
anova(model.reduced, model.full) #The p-value is quite large; thus, the reduced
summary(model.reduced)
plot(model.reduced)
vif(model.reduced)
AIC(model.full,    #Model with all variables.
model2,        #Model with all variables EXCEPT Illiteracy.
model.reduced) #Model with all variables EXCEPT Illiteracy, Area, and Income.
BIC(model.full,
model2,
model.reduced) #Both the minimum AIC and BIC values appear alongside the
model.empty = lm(Life.Exp ~ 1, data = states) #The model with an intercept ONLY.
model.full = lm(Life.Exp ~ ., data = states) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
library(MASS) #The Modern Applied Statistics library.
forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
backwardAIC = step(model.full, scope, direction = "backward", k = 2)
bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)
bothAIC.full = step(model.full, scope, direction = "both", k = 2)
forwardBIC = step(model.empty, scope, direction = "forward", k = log(50))
backwardBIC = step(model.full, scope, direction = "backward", k = log(50))
bothBIC.empty = step(model.empty, scope, direction = "both", k = log(50))
bothBIC.full = step(model.full, scope, direction = "both", k = log(50))
summary(forwardAIC)
plot(forwardAIC)
influencePlot(forwardAIC)
vif(forwardAIC)
avPlots(forwardAIC)
confint(forwardAIC)
forwardAIC$fitted.values #Returns the fitted values.
newdata = data.frame(Murder = c(1.5, 7.5, 12.5),
HS.Grad = c(60, 50, 40),
Frost = c(75, 55, 175),
Population = c(7500, 554, 1212))
predict(forwardAIC, newdata, interval = "confidence") #Construct confidence intervals
predict(forwardAIC, newdata, interval = "prediction") #Construct prediction invervals
library(caret)
train(Life.Exp~., data=states,
method="glmStepAIC", k=2,
trControl=trainControl(method="none"))
train(Life.Exp~., data=states,
method="glmStepAIC", k=2,
trControl=trainControl(method="none"), preProc=c('center', 'scale'))
tests = read.table("[04] Test Scores.txt")
summary(tests)
sd(tests$Test.Score)
sd(tests$Hours.Studied)
cor(tests$Test.Score, tests$Hours.Studied)
plot(tests$Hours.Studied, tests$Test.Score)
model.simple = lm(Test.Score ~ Hours.Studied, data = tests)
summary(model.simple) #Investigating the model and assessing some diagnostics.
plot(model.simple)
influencePlot(model.simple)
newdata = data.frame(Hours.Studied = seq(1, 3, length = 100))
conf.band = predict(model.simple, newdata, interval = "confidence")
pred.band = predict(model.simple, newdata, interval = "prediction")
plot(tests$Hours.Studied, tests$Test.Score,
xlab = "Hours Studied", ylab = "Test Score",
main = "Simple Linear Regression Model\nTests Dataset")
abline(model.simple, lty = 2)
lines(newdata$Hours.Studied, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Hours.Studied, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Hours.Studied, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Hours.Studied, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
model.quadratic = lm(Test.Score ~ Hours.Studied + I(Hours.Studied^2), data = tests)
summary(model.quadratic) #Investigating the model and assessing some diagnostics.
plot(model.quadratic)
influencePlot(model.quadratic)
conf.band = predict(model.quadratic, newdata, interval = "confidence")
pred.band = predict(model.quadratic, newdata, interval = "prediction")
plot(tests$Hours.Studied, tests$Test.Score,
xlab = "Hours Studied", ylab = "Test Score",
main = "Quadratic Regression Model\nTests Dataset")
lines(tests$Hours.Studied[order(tests$Hours.Studied)],
model.quadratic$fitted.values[order(tests$Hours.Studied)], lty = 2)
lines(newdata$Hours.Studied, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Hours.Studied, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Hours.Studied, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Hours.Studied, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
model.factor = lm(Test.Score ~ Hours.Studied + Gender, data = tests)
summary(model.factor) #Investigating the model and assessing some diagnostics.
plot(model.factor)
col.vec = c(rep("pink", 250), rep("blue", 250))
plot(tests$Hours.Studied, tests$Test.Score, col = col.vec,
xlab = "Hours Studied", ylab = "Test Score",
main = "Linear Regression Model w/ Factor\nTests Dataset")
abline(model.factor$coefficients[1], #Intercept for females.
model.factor$coefficients[2], #Slope for females.
lwd = 3, lty = 2, col = "pink")
abline(model.factor$coefficients[1] + model.factor$coefficients[3], #Intercept for males.
model.factor$coefficients[2], #Slope for males.
lwd = 3, lty = 2, col = "blue")
legend("topleft", c("Female Regression", "Male Regression"),
lwd = 3, lty = 2, col = c("pink", "blue"))
setwd("C:/Users/dsp21/NYDS/Project/Web_Scr_indeed_glassdoor/WebScrapingNYDS_indeed_git/R_viz")
this.dir <- dirname(parent.frame(2)$ofile)
this.dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
this.dir
setwd(this.dir)
knitr::opts_chunk$set(echo = TRUE,
cache = TRUE,
warning = FALSE,
message = FALSE,
tidy=FALSE,
fig.height=6,
fig.width=10)
this.dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
#https://stackoverflow.com/questions/13672720/r-command-for-setting-working-directory-to-source-file-location
setwd(this.dir)
## load training data
comp_train <- read.csv("../Data/co_rev_sample_filtered.csv",
header = TRUE,
na.strings = "",
stringsAsFactors = FALSE)
comp_train2 = read.csv("../Data/co_rev_sample_filtered_pt2.csv",
header = TRUE,
na.strings = "",
stringsAsFactors = FALSE)
comp_train = rbind(comp_train, comp_train2)
rm(comp_train2)
## load test data
# house_test <- read.csv("./test.csv",
#                         header = TRUE,
#                         na.strings = "",
#                         stringsAsFactors = FALSE)
dim(comp_train)
#dim(house_test)
str(comp_train)
topct <- function(x) { as.numeric( sub("\\D*([0-9.]+)\\D*","\\1",x) )/100 }
comp_train[['ceo_approval_pct']] = topct(comp_train[['ceo_approval_pct']])
comp_train[['ceo_approval_pct']]
comp_train[['rev_date_num']] = as.Date(comp_train[['review_date']], '%B %d, %Y')
sub = comp_train %>% filter(is.na(Quick.paced), is.na(Slow.paced))
comp_train$Slow.paced = NULL
sub = comp_train %>% filter(!is.na(Relaxed), !is.na(Cut.throat))
library(dplyr)
sub = comp_train %>% filter(is.na(Quick.paced), is.na(Slow.paced))
sub = comp_train %>% filter(!is.na(Relaxed), !is.na(Cut.throat))
View(sub)
comp_train$X <- NULL
rownames(comp_train) <- comp_train$review_id
comp_train$review_id <- NULL
save(comp_train, file = "../Data/co_rev_loaded.RData")
View(comp_train)
unique(comp_train$company_industry)
library(ggplot2)
library(gridExtra)
library(tabplot)
library(lsr)
library(corrplot)
library(plotly)
rm(list = ls())
load("../Data/co_rev_loaded.RData")
corrplot(corM, method = "color", order = "hclust")
corM = cor(comp_train[to_chart], use = "complete.obs")
comp_train %>%
group_by(company_industry) %>%
summarise(n())
comp_train %>%
group_by(company_industry) %>%
summarise(n()) %>% arrange(desc('n()'))
comp_train %>%
group_by(company_industry) %>%
summarise(n()) %>% arrange(desc(n()))
comp_train %>%
group_by(company_industry) %>%
summarise(n = n()) %>% arrange(desc(n))
g = ggplot(comp_train %>%
group_by(company_industry) %>%
summarise(n = n()) %>% arrange(desc(n))) +
geom_bar(aes(x = company_industry, y = n), stat = 'identity')
ggplotly(g)
g = ggplot(comp_train %>%
group_by(company_industry) %>%
summarise(n = n()) %>% arrange(desc(n))) +
geom_bar(aes(x = reorder(company_industry,n), y = n), stat = 'identity')
g = ggplot(comp_train %>%
group_by(company_industry) %>%
summarise(n = n()) %>% arrange(desc(n))) +
geom_bar(aes(x = reorder(company_industry,n), y = n), stat = 'identity') +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplotly(g)
g = ggplot(comp_train %>%
group_by(company_industry) %>%
summarise(n = n()) %>% arrange(desc(n))) +
geom_bar(aes(x = reorder(company_industry,-n), y = n), stat = 'identity') +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplotly(g)
g = ggplot(comp_train %>%
group_by(company_industry) %>%
summarise(n = n()) %>% arrange(desc(n))) +
geom_bar(aes(x = reorder(company_industry,-n), y = n), stat = 'identity') +
theme(axis.text.x = element_text(angle = 45, hjust = 2))
ggplotly(g)
g = ggplot(comp_train %>%
group_by(company_industry) %>%
summarise(n = n()) %>% arrange(desc(n))) +
geom_bar(aes(x = reorder(company_industry,-n), y = n), stat = 'identity') +
theme(axis.text.x = element_text(angle = 45, hjust = 3))
ggplotly(g)
g = ggplot(comp_train %>%
group_by(company_industry) %>%
summarise(n = n()) %>% arrange(desc(n))) +
geom_bar(aes(x = reorder(company_industry,-n), y = n), stat = 'identity') +
theme(axis.text.x = element_text(angle = 45, hjust = 3))
ggplotly(g)
numeric_features <- names(comp_train)[sapply(comp_train, is.numeric)]
print(numeric_features)
review_spec_cols = c('agg_rating','comp_ben_rating', 'culture_rating', 'jobsec_advancement_rating',
'management_rating', 'work_life_rating', 'helpful_downvote_count',
'helpful_upvote_count','review_date', 'review_title',
'reviewer_company_empl_status', 'reviewer_job_location',
'reviewer_job_title', 'main_text', 'con_text', 'pro_text','rev_date_num')
numeric_features[!(numeric_features %in% c('review_id'))]
company_cols = names(comp_train)[!(names(comp_train) %in% review_spec_cols)]
numeric_features <- names(comp_train)[sapply(comp_train, is.numeric)]
print(numeric_features)
review_spec_cols = c('agg_rating','comp_ben_rating', 'culture_rating', 'jobsec_advancement_rating',
'management_rating', 'work_life_rating', 'helpful_downvote_count',
'helpful_upvote_count','review_date', 'review_title',
'reviewer_company_empl_status', 'reviewer_job_location',
'reviewer_job_title', 'main_text', 'con_text', 'pro_text','rev_date_num')
company_cols = names(comp_train)[!(names(comp_train) %in% review_spec_cols)]
grid.arrange(ggplot(comp_train) +
geom_histogram(aes(agg_rating), bins = 5),
ggplot(comp_train) +
geom_histogram(aes(company_overall_rating), bins = 15),
ncol = 2)
review_spec_numeric = review_spec_cols[review_spec_cols %in% numeric_features]
?summarise_if
by_co = comp_train %>%
group_by(company_name) %>%
summarise_if(function (x) {x %in% review_spec_numeric}, mean)
by_co = comp_train %>%
group_by(company_name) %>%
summarise_if(., function (x) {x %in% review_spec_numeric}, mean)
by_co = comp_train %>%
group_by(company_name) %>%
summarise_if(., function (x) {names(x) %in% review_spec_numeric}, mean)
by_co = comp_train %>%
group_by(company_name) %>%
summarise_if(review_spec_numeric, mean)
by_co = comp_train %>%
group_by(company_name) %>%
summarise_at(review_spec_numeric, mean, na.rm = T)
by_co = comp_train %>%
group_by(company_name) %>%
summarise_at(numeric_features, mean, na.rm = T)
by_co
grid.arrange(ggplot(comp_train) +
geom_histogram(aes(agg_rating), bins = 5),
ggplot(comp_train) +
geom_histogram(aes(company_overall_rating), bins = 15),
ncol = 2)
plot(by_co$agg_rating, by_co$company_overall_rating)
ggplot(by_co) +
geom_point(aes(x = company_overall_rating, y = agg_rating, fill = company_industry), position = "jitter")
by_co = comp_train %>%
group_by(company_industry, company_name) %>%
summarise_at(numeric_features, mean, na.rm = T)
ggplot(by_co) +
geom_point(aes(x = company_overall_rating, y = agg_rating, fill = company_industry), position = "jitter")
ggplot(by_co) +
geom_point(aes(x = company_overall_rating, y = agg_rating, fill = company_industry), position = "jitter") +
geom_smooth(method = "lm")
ggplot(by_co, aes(x = company_overall_rating, y = agg_rating, fill = company_industry)) +
geom_point(position = "jitter") +
geom_smooth(method = "lm")
ggplot(by_co, aes(x = company_overall_rating, y = agg_rating)) +
geom_point(aes(fill = company_industry), position = "jitter") +
geom_smooth(method = "lm") + scale_color_brewer(palette = 'Blues')
ggplot(by_co, aes(x = company_overall_rating, y = agg_rating)) +
geom_point(aes(color = company_industry), position = "jitter") +
geom_smooth(method = "lm") + scale_color_brewer(palette = 'Blues')
ggplot(by_co, aes(x = company_overall_rating, y = agg_rating)) +
geom_point(aes(color = company_industry), position = "jitter") +
geom_smooth(method = "lm")
ggplotly(ggplot(by_co, aes(x = company_overall_rating, y = agg_rating)) +
geom_point(aes(color = company_industry), position = "jitter") +
geom_smooth(method = "lm"))
ggplotly(ggplot(by_co, aes(x = company_overall_rating, y = agg_rating)) +
geom_point( position = "jitter") + #could add 'aes(color = company_industry),'
geom_smooth(method = "lm"))
comp_train$month = months(comp_train$rev_date_num)
comp_train$year = format(comp_train$rev_date_num, '%Y')
comp_train$year = format(comp_train$rev_date_num, '%Y')
comp_train$weekday = weekdays(comp_train$rev_date_num)
save(comp_train, file = "../Data/co_rev_loaded.RData")
library(ggplot2)
library(tidytext)
library(dplyr)
library(stringr)
library(plotly)
text_df = comp_train
text_cols = ['review_title','main_text','reviewer_job_title', 'pro_text', 'con_text']
text_cols = c('review_title','main_text','reviewer_job_title', 'pro_text', 'con_text')
tidy_text = text_df[text_cols]
View(tidy_text)
tidy_text = melt(tidy_text)
library(reshape2)
tidy_text = melt(tidy_text)
View(head(tidy_text))
?melt
tidy_text['review_id'] = row.names(tidy_text)
sum(tidy_text$review_id %in% 126990:127000)
tidy_text = melt(tidy_text, id = review_id)
tidy_text = melt(tidy_text, id = 'review_id')
View(head(tidy_text))
View(tidy_text)
token = 'word'
library(tidytext)
install.packages('tidytext')
compid <- mutate(comp_train[numeric_features], id=as.numeric(rownames(comp_train)))
melt_df = melt(compid, id = 'id')
ggplot(melt_df) + geom_histogram(aes(x = value), bins = 10) + facet_wrap(~variable, scales="free")
ggp = ggpairs(comp_train[,review_numerics],
lower = list(continuous = 'density'))
to_chart = review_numerics[!grepl("^help",review_numerics)]
to_chart = review_spec_numerics[!grepl("^help",review_spec_numeric)]
to_chart = review_spec_numeric[!grepl("^help",review_spec_numeric)]
corM = cor(comp_train[to_chart], use = "complete.obs")
corrplot(corM, method = "color", order = "hclust")
review_numerics = numeric_features[numeric_features %in% review_spec_cols]
to_chart = review_spec_numeric[!grepl("^help",review_spec_numeric)]
ggp = ggpairs(comp_train[,review_numerics],
lower = list(continuous = 'density'))
library(GGally)
ggp = ggpairs(comp_train[,review_numerics],
lower = list(continuous = 'density'))
ggp
ggp = ggpairs(comp_train[,to_chart],
lower = list(continuous = 'density'))
ggp
data(stop_words)
library(tidytext)
library(stringr)
data(stop_words)
tidy_text_dropstop <- tidy_text %>%
anti_join(stop_words)
tidy_text = tidy_text %>%
unnest_tokens(word, value)
View(head(tidy_text))
tidy_text = tidy_text %>%
unnest_tokens(word, value)
save(tidy_text, "tidy_text.Rda")
setwd("C:/Users/dsp21/NYDS/Project/Web_Scr_indeed_glassdoor/WebScrapingNYDS_indeed_git/R_viz")
save(tidy_text, file = "tidy_text.Rda")
